# -*- coding: utf-8 -*-
"""milestone3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nZIFYYrBj7XHZIToXemMQsh8y_6qW6qN
"""

import pandas as pd
import numpy as np
import seaborn as sns
from string import ascii_letters
import matplotlib.pyplot as plt
import datetime as dt
import requests
from lxml import html
import math
import json
import re
import os
from google.colab import drive
drive.mount('/content/drive')

# Create dataframes from csv files

books_data_df = pd.read_csv('/content/drive/MyDrive/CIS 550 Project Data/books_data.csv')
books_reviews_df = pd.read_csv('/content/drive/MyDrive/CIS 550 Project Data/Books_rating.csv')

books_data_df.head()

books_reviews_df.head()

def get_expanded_column(df, column_name):
  df[column_name] = df[column_name].astype(str)
  df[column_name + '_expanded'] = df[column_name].apply(lambda x: x.split(','))
  df = df.explode(column_name + '_expanded')
  df[column_name + '_expanded'] = df[column_name + '_expanded'].apply(lambda x: x.replace('[', '').replace(']', '').replace('\'', '').replace('"', '').strip())

  return df

def fix_dates(date_str):
  """
  Converts year-only dates (e.g., '1996') to 'YYYY-01-01' format
  and year-month dates (e.g., '1996-05') to 'YYYY-MM-01' format.
  Cleans the date string by removing trailing non-alphanumeric characters.
  """
  # Clean the date string using regex to remove trailing non-alphanumeric characters
  date_str = re.sub(r"[^a-zA-Z0-9]+$", "", str(date_str))

  try:
    # Try parsing with the original format first
    return pd.to_datetime(date_str, format='%Y-%m-%d', errors='raise').strftime('%Y-%m-%d')
  except ValueError:
    # If it's a year-only or year-month date, convert it
    try:
        return pd.to_datetime(date_str, format='%Y-%m', errors='raise').strftime('%Y-%m-%d')  # Try year-month format first
    except ValueError:
        try:
            return pd.to_datetime(date_str, format='%Y', errors='raise').strftime('%Y-%m-%d')  # Try year-only format
        except ValueError:
            # Still invalid, return NaT to indicate missing date
            return pd.NaT

def convert_helpfulness_to_float(helpfulness_str):
  """
  Converts helpfulness string (e.g., '7/7', '14/19') to a float representing the ratio.
  Returns NaN if the string is not in the expected format or if the denominator is 0 (except for '0/0', which is converted to 0.0).
  """
  try:
    numerator, denominator = map(int, helpfulness_str.split('/'))
    if denominator == 0:
      if numerator == 0:  # Special case for '0/0'
        return 0.0
      else:
        return float('nan')  # Avoid division by zero
    else:
      return numerator / denominator
  except (ValueError, AttributeError):
    return float('nan')  # Handle cases where the string is not in the expected format

## PUBLISHER ##
'''
  PublisherId 		INT AUTO_INCREMENT,
	PublisherName	VARCHAR (255) NOT NULL
	PRIMARY KEY (PublisherId)
'''

publisher_df = books_data_df['publisher'].dropna().unique()
publisher_df = pd.DataFrame(publisher_df, columns=['PublisherName'])
publisher_df = publisher_df.reset_index()
publisher_df['index'] = publisher_df['index'] + 1
publisher_df = publisher_df.rename(columns={'index': 'PublisherId'})
print(publisher_df)

# Join book review data with books data on title to get BookId into Books df
books_id_data_df = pd.merge(books_data_df, books_reviews_df, on='Title', how='left')

# Drop books with missing categories information from dataset
books_id_data_df = books_id_data_df.dropna(subset=['categories'])

## BOOKS ##

# Book(BookId, Title, PublisherId, PublishedDate, RatingsCount, Price, InfoLink, PreviewLink, Image)
books_data_df_3 = books_id_data_df[['Id', 'Title', 'publisher', 'publishedDate', 'ratingsCount', 'Price', 'infoLink', 'previewLink', 'image']]
books_data_df_3 = books_data_df_3.drop_duplicates(subset='Id', keep='first').reset_index(drop=True)

# Join publisher data w/ books data to get publisherId
books_data_df_3 = pd.merge(books_data_df_3, publisher_df, left_on='publisher', right_on='PublisherName', how='left')
books_df = books_data_df_3[['Id', 'Title', 'PublisherId', 'publishedDate', 'ratingsCount', 'Price', 'infoLink', 'previewLink', 'image']]
books_df = books_df.rename(columns={'Id': 'BookId', 'publishedDate': 'PublishedDate', 'ratingsCount':'RatingsCount', 'infoLink': 'InfoLink', 'previewLink':'PreviewLink', 'image':'Image'})

# Drop null values
books_df = books_df.dropna(subset=['Price', 'ratingsCount']).reset_index(drop=True)
print(books_df)

books_df['PublishedDate'] = books_df['PublishedDate'].apply(fix_dates)
books_df['PublishedDate'] = pd.to_datetime(books_df['PublishedDate'])

print(books_df)

## AUTHOR ##

# Explode authors column in books data
book_author_df = get_expanded_column(books_id_data_df.copy(), 'authors') # Create a copy to avoid modifying the original DataFrame

# Create AuthorId column + other data manipulation
# Access the expanded column from the exploded DataFrame
unique_authors = book_author_df['authors_expanded'].dropna().unique()
author_df = pd.DataFrame(unique_authors, columns=['authors_expanded'])
author_df.reset_index(inplace=True)
author_df['index'] = author_df['index'] + 1
author_df = author_df.rename(columns={'index': 'AuthorId', 'authors_expanded':'Name'})
print(author_df)

book_author_df.head()

## BOOK AUTHOR ##

# Merge book_author_df w/ authors
book_author_df = pd.merge(book_author_df, author_df, left_on='authors_expanded', right_on='Name', how='left')

# Keep only the columns we need
book_author_df = book_author_df[['Id', 'AuthorId']]
book_author_df = book_author_df.drop_duplicates(subset=['Id', 'AuthorId'], keep='first')
book_author_df = book_author_df.rename(columns={'Id': 'BookId'})
book_author_df = book_author_df.reset_index(drop=True)
print(book_author_df)

## CATEGORY ##

# Expand categories column
book_category_df = get_expanded_column(books_id_data_df.copy(), 'categories')

# Create Category dataframe
unique_categories = book_category_df['categories_expanded'].dropna().unique()
category_df = pd.DataFrame(unique_categories, columns=['categories_expanded'])
category_df.reset_index(inplace=True)
category_df['index'] = category_df['index'] + 1
category_df = category_df.rename(columns={'index': 'CategoryId', 'categories_expanded':'Type'})
print(category_df)

## BOOK CATEGORY ##

# Merge book_category_df w/ category
book_category_df = pd.merge(book_category_df, category_df, left_on='categories_expanded', right_on='Type', how='left')

# Keep only the columns we need
book_category_df = book_category_df[['Id', 'CategoryId']]
book_category_df = book_category_df.drop_duplicates(subset=['Id', 'CategoryId'], keep='first')
book_category_df = book_category_df.rename(columns={'Id': 'BookId'})
book_category_df = book_category_df.reset_index(drop=True)
print(book_category_df)

## REVIEW ##

review_df = books_reviews_df[['Id', 'User_id', 'review/text', 'review/summary', 'review/helpfulness', 'review/score', 'review/time']]
review_df = review_df.rename(columns={'Id': 'BookId', 'User_id': 'UserId', 'review/text': 'Text', 'review/summary': 'Summary', 'review/helpfulness': 'Helpfulness', 'review/score': 'Score', 'review/time': 'Time'})
review_df['Time'] = pd.to_datetime(review_df['Time'], unit='s')
review_df = review_df.reset_index(drop=True)
print(review_df)

review_df['Helpfulness_Float'] = review_df['Helpfulness'].apply(convert_helpfulness_to_float)
print(review_df)

review_df = review_df[['BookId', 'UserId', 'Text', 'Summary', 'Helpfulness_Float', 'Score', 'Time']]
review_df = review_df.rename(columns={'Helpfulness_Float':'Helpfulness'})
review_df = review_df.reset_index(drop=True)
print(review_df)

review_df = review_df.drop(columns=['Text'])
review_df = review_df.reset_index(drop=True)
print(review_df)

review_df = review_df.dropna(subset=['BookId', 'UserId']).reset_index(drop=True)
print(review_df)

review_df = review_df.drop_duplicates(subset=['BookId', 'UserId', 'Time', 'Helpfulness', 'Score', 'Summary']).reset_index()
print(review_df)

review_df['index'] = review_df['index'] + 1
review_df = review_df.rename(columns={'index': 'ReviewId'})
print(review_df)

## USER ##

user_df = books_reviews_df[['User_id', 'profileName']]
user_df = user_df.drop_duplicates(subset='User_id', keep='first')
user_df = user_df.rename(columns={'User_id': 'UserId', 'profileName':'ProfileName'})
print(user_df)

user_df = user_df.dropna(subset='UserId').reset_index(drop=True)
print(user_df)

# Export cleaned dataframes into CSV files

'''
# Author, BookAuthor, Category, BookCategory, Book, Publisher, Review, User
author_df.to_csv('/content/drive/MyDrive/CIS 550 Project Data/author.csv', index=False)
book_author_df.to_csv('/content/drive/MyDrive/CIS 550 Project Data/book_author.csv', index=False)
category_df.to_csv('/content/drive/MyDrive/CIS 550 Project Data/category.csv', index=False)
book_category_df.to_csv('/content/drive/MyDrive/CIS 550 Project Data/book_category.csv', index=False)
books_df.to_csv('/content/drive/MyDrive/CIS 550 Project Data/book.csv', index=False)
publisher_df.to_csv('/content/drive/MyDrive/CIS 550 Project Data/publisher.csv', index=False)
review_df.to_csv('/content/drive/MyDrive/CIS 550 Project Data/review.csv', index=False)
user_df.to_csv('/content/drive/MyDrive/CIS 550 Project Data/user.csv', index=False)
'''

review_df.to_csv('/content/drive/MyDrive/CIS 550 Project Data/review.csv', index=False)
